{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f79baf9",
   "metadata": {},
   "source": [
    "# ADS 509 Assignment 2.1: Tokenization, Normalization, Descriptive Statistics \n",
    "\n",
    "This notebook holds Assignment 2.1 for Module 2 in ADS 509, Applied Text Mining. Work through this notebook, writing code and answering questions where required. \n",
    "\n",
    "In the previous assignment you pulled lyrics data on two artists. In this assignment we explore this data set and a pull from the now-defunct Twitter API for the artists Cher and Robyn.  If, for some reason, you did not complete that previous assignment, data to use for this assignment can be found in the assignment materials section of Canvas. \n",
    "\n",
    "This assignment asks you to write a short function to calculate some descriptive statistics on a piece of text. Then you are asked to find some interesting and unique statistics on your corpora. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e2d096b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import emoji\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "sw = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6b555ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add any additional import statements you need here\n",
    "\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "923b5a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change `data_location` to the location of the folder on your machine.\n",
    "data_location = \"/Users/clairebentzen/Desktop/MDAS/ADS 509 - Applied Text Mining/Module 2/Assignment2.1/M1 Results/\"\n",
    "\n",
    "# These subfolders should still work if you correctly stored the \n",
    "# data from the Module 1 assignment\n",
    "twitter_folder = \"twitter/\"\n",
    "lyrics_folder = \"lyrics/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "06522af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def descriptive_stats(tokens, num_tokens = 5, verbose=True) :\n",
    "    \"\"\"\n",
    "        Given a list of tokens, print number of tokens, number of unique tokens, \n",
    "        number of characters, lexical diversity (https://en.wikipedia.org/wiki/Lexical_diversity), \n",
    "        and num_tokens most common tokens. Return a list with the number of tokens, number\n",
    "        of unique tokens, lexical diversity, and number of characters. \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Fill in the correct values here. \n",
    "    num_tokens = len(tokens)\n",
    "    num_unique_tokens = len(set(tokens))\n",
    "    lexical_diversity = num_unique_tokens / num_tokens\n",
    "    num_characters = sum(len(token) for token in tokens)\n",
    "    \n",
    "    if verbose :        \n",
    "        print(f\"There are {num_tokens} tokens in the data.\")\n",
    "        print(f\"There are {num_unique_tokens} unique tokens in the data.\")\n",
    "        print(f\"There are {num_characters} characters in the data.\")\n",
    "        print(f\"The lexical diversity is {lexical_diversity:.3f} in the data.\")\n",
    "    \n",
    "        # Print the five most common tokens\n",
    "        top_five = Counter(tokens).most_common(5)\n",
    "        print(top_five)\n",
    "        \n",
    "    return([num_tokens, num_unique_tokens,\n",
    "            lexical_diversity,\n",
    "            num_characters])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "59dcf058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 13 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 55 characters in the data.\n",
      "The lexical diversity is 0.692 in the data.\n",
      "[('text', 3), ('here', 2), ('example', 2), ('is', 1), ('some', 1)]\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"here is some example text with other example text here in this text\"\"\".split()\n",
    "assert(descriptive_stats(text, verbose=True)[0] == 13)\n",
    "assert(descriptive_stats(text, verbose=False)[1] == 9)\n",
    "assert(abs(descriptive_stats(text, verbose=False)[2] - 0.69) < 0.02)\n",
    "assert(descriptive_stats(text, verbose=False)[3] == 55)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e7e1a2",
   "metadata": {},
   "source": [
    "Q: Why is it beneficial to use assertion statements in your code? \n",
    "\n",
    "A: It is beneficial to use assertion statements in your code to ensure that certain conditions are met before continuing on in the code. For example, if a certain value is expected, an assertion statement can be used to check that the expected value is met before anything else is performed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3bf93e",
   "metadata": {},
   "source": [
    "## Data Input\n",
    "\n",
    "Now read in each of the corpora. For the lyrics data, it may be convenient to store the entire contents of the file to make it easier to inspect the titles individually, as you'll do in the last part of the assignment. In the solution, I stored the lyrics data in a dictionary with two dimensions of keys: artist and song. The value was the file contents. A data frame would work equally well. \n",
    "\n",
    "For the Twitter data, we only need the description field for this assignment. Feel free all the descriptions read it into a data structure. In the solution, I stored the descriptions as a dictionary of lists, with the key being the artist. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "37d70801",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song</th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>robyn</td>\n",
       "      <td>includemeout</td>\n",
       "      <td>\"Include Me Out\"\\n\\n\\n\\nIt is really very simp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>robyn</td>\n",
       "      <td>electric</td>\n",
       "      <td>\"Electric\"\\n\\n\\n\\nElectric...\\n\\nIt's electric...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>robyn</td>\n",
       "      <td>beach2k20</td>\n",
       "      <td>\"Beach 2K20\"\\n\\n\\n\\n(So you wanna go out?\\nHow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>robyn</td>\n",
       "      <td>lovekills</td>\n",
       "      <td>\"Love Kills\"\\n\\n\\n\\nIf you're looking for love...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>robyn</td>\n",
       "      <td>timemachine</td>\n",
       "      <td>\"Time Machine\"\\n\\n\\n\\nHey, what did I do?\\nCan...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  artist          song                                             lyrics\n",
       "0  robyn  includemeout  \"Include Me Out\"\\n\\n\\n\\nIt is really very simp...\n",
       "1  robyn      electric  \"Electric\"\\n\\n\\n\\nElectric...\\n\\nIt's electric...\n",
       "2  robyn     beach2k20  \"Beach 2K20\"\\n\\n\\n\\n(So you wanna go out?\\nHow...\n",
       "3  robyn     lovekills  \"Love Kills\"\\n\\n\\n\\nIf you're looking for love...\n",
       "4  robyn   timemachine  \"Time Machine\"\\n\\n\\n\\nHey, what did I do?\\nCan..."
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the lyrics data\n",
    "# Specify pathway to lyrics folder\n",
    "lyrics_path = data_location + lyrics_folder\n",
    "\n",
    "# Create a dataframe to store results\n",
    "lyrics_df = pd.DataFrame(columns=['artist', 'song', 'lyrics'])\n",
    "\n",
    "# Iterate through each file in the lyrics folder\n",
    "for artist in os.listdir(lyrics_path):\n",
    "    artist_path = os.path.join(lyrics_path, artist)\n",
    "    \n",
    "    # Iterate through each file in the artist folders\n",
    "    for song in os.listdir(artist_path):\n",
    "        song_path = os.path.join(artist_path, song)\n",
    "        rem_prefix = song.removeprefix(f'{artist}_')\n",
    "        song_title = rem_prefix.removesuffix('.txt')\n",
    "\n",
    "        # Open and read the contents of the file (song)\n",
    "        with open(song_path, 'r') as file:\n",
    "            contents = file.read()\n",
    "            # Prepare data to add to dataframe\n",
    "            data = {'artist': artist, 'song': song_title, 'lyrics': contents}\n",
    "            # The df.append() function is deprecated, so we will ignore warnings here\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter('ignore')\n",
    "                # Append row of data to lyrics_df\n",
    "                lyrics_df = lyrics_df.append(data, ignore_index=True)\n",
    "                \n",
    "# Print first 5 records\n",
    "lyrics_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "debcac5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the twitter data\n",
    "# Specify pathway to twitter folder\n",
    "twitter_path = data_location + twitter_folder\n",
    "\n",
    "# Create a dataframe to store results\n",
    "twitter_df = pd.DataFrame(columns=['artist', 'description'])\n",
    "\n",
    "# Iterate through each file in the twitter folder where file name ends in _data.txt\n",
    "for file_name in os.listdir(twitter_path):\n",
    "    if file_name.endswith('_data.txt'):\n",
    "        file_path = os.path.join(twitter_path, file_name)\n",
    "        # Read tab delimited file\n",
    "        temp_df = pd.read_csv(file_path, delimiter = '\\t', on_bad_lines='skip')\n",
    "        \n",
    "        # Save artist name as the string prior to the first _\n",
    "        artist_name = file_name.split('_', 1)[0]\n",
    "        # Add artist as a column\n",
    "        temp_df['artist'] = artist_name\n",
    "        \n",
    "        # Subset only the artist and description columns\n",
    "        temp_df = temp_df[['artist', 'description']]\n",
    "        \n",
    "        # Append new columns to twitter_df\n",
    "        twitter_df = pd.concat([twitter_df, temp_df], ignore_index=True)\n",
    "        \n",
    "# Print first 5 records\n",
    "twitter_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5f3b12",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "Now clean and tokenize your data. Remove punctuation chacters (available in the `punctuation` object in the `string` library), split on whitespace, fold to lowercase, and remove stopwords. Store your cleaned data, which must be accessible as an interable for `descriptive_stats`, in new objects or in new columns in your data frame. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "71c73d86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!',\n",
       " '\"',\n",
       " '#',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " '+',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " ':',\n",
       " ';',\n",
       " '<',\n",
       " '=',\n",
       " '>',\n",
       " '?',\n",
       " '@',\n",
       " '[',\n",
       " '\\\\',\n",
       " ']',\n",
       " '^',\n",
       " '_',\n",
       " '`',\n",
       " '{',\n",
       " '|',\n",
       " '}',\n",
       " '~'}"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuation = set(punctuation) # speeds up comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "c05ddbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning Function\n",
    "def data_cleaning(text):\n",
    "    text = str(text)\n",
    "    \n",
    "    # Fold to lowercase\n",
    "    fold_lower = text.lower()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    remove_punctuation = ''.join(char for char in fold_lower if char not in punctuation)\n",
    "    \n",
    "    # Split on whitespace\n",
    "    split = remove_punctuation.split()\n",
    "    \n",
    "    \n",
    "    # Remove stopwords\n",
    "    clean_text = [word for word in split if word not in sw]\n",
    "    \n",
    "    return ' '.join(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "b327033a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean twitter data\n",
    "twitter_df['cleaned_description'] = twitter_df['description'].apply(data_cleaning)\n",
    "twitter_df['artist'] = twitter_df['artist'].replace('robynkonichiwa', 'robyn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "e0f22e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean lyric data\n",
    "lyrics_df['cleaned_lyrics'] = lyrics_df['lyrics'].apply(data_cleaning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dd0179",
   "metadata": {},
   "source": [
    "## Basic Descriptive Statistics\n",
    "\n",
    "Call your `descriptive_stats` function on both your lyrics data and your twitter data and for both artists (four total calls). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "7f982ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 35916 tokens in the data.\n",
      "There are 3703 unique tokens in the data.\n",
      "There are 172634 characters in the data.\n",
      "The lexical diversity is 0.103 in the data.\n",
      "[('love', 1004), ('im', 513), ('know', 486), ('dont', 440), ('youre', 333)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[120022112, 12212, 0.00010174791791699183, 120022112]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cher Lyrics Stats\n",
    "# Subset cher lyrics\n",
    "cher_lyrics = lyrics_df[lyrics_df['artist'] == 'cher']\n",
    "# Concat cleaned lyrics into one string\n",
    "cher_lyrics_str = cher_lyrics['cleaned_lyrics'].str.cat(sep=' ')\n",
    "\n",
    "# Calculate descriptive stats\n",
    "cher_lyrics_desc = descriptive_stats(cher_lyrics_str.split())\n",
    "cher_twitter_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "390b16f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 15227 tokens in the data.\n",
      "There are 2156 unique tokens in the data.\n",
      "There are 73787 characters in the data.\n",
      "The lexical diversity is 0.142 in the data.\n",
      "[('know', 308), ('dont', 301), ('im', 299), ('love', 275), ('got', 251)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[351839, 177472, 0.504412529594502, 11244107]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Robyn Lyrics Stats\n",
    "# Subset robyn lyrics\n",
    "robyn_lyrics = lyrics_df[lyrics_df['artist'] == 'robyn']\n",
    "# Concat cleaned lyrics into one string\n",
    "robyn_lyrics_str = robyn_lyrics['cleaned_lyrics'].str.cat(sep=' ')\n",
    "\n",
    "# Calculate descriptive stats\n",
    "robyn_lyrics_desc = descriptive_stats(robyn_lyrics_str.split())\n",
    "robyn_twitter_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "f0bbedd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 18102661 tokens in the data.\n",
      "There are 1697994 unique tokens in the data.\n",
      "There are 101904931 characters in the data.\n",
      "The lexical diversity is 0.094 in the data.\n",
      "[('nan', 1953916), ('love', 214529), ('im', 139037), ('life', 122900), ('music', 88168)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[18102661, 1697994, 0.09379803333885554, 101904931]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cher Twitter Stats\n",
    "# Subset cher twitter data\n",
    "cher_twitter = twitter_df[twitter_df['artist'] == 'cher']\n",
    "# Concat descriptions into one string\n",
    "cher_twitter_str = cher_twitter['cleaned_description'].str.cat(sep=' ')\n",
    "\n",
    "# Calculate descriptive stats\n",
    "cher_twitter_desc = descriptive_stats(cher_twitter_str.split())\n",
    "cher_twitter_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "195a2c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1703000 tokens in the data.\n",
      "There are 271325 unique tokens in the data.\n",
      "There are 9891691 characters in the data.\n",
      "The lexical diversity is 0.159 in the data.\n",
      "[('nan', 164845), ('music', 15160), ('love', 11683), ('im', 9052), ('och', 7922)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1703000, 271325, 0.15932178508514386, 9891691]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Robyn Twitter Stats\n",
    "# Subset robyn twitter data\n",
    "robyn_twitter = twitter_df[twitter_df['artist'] == 'robyn']\n",
    "# Concat descriptions into one string\n",
    "robyn_twitter_str = robyn_twitter['cleaned_description'].str.cat(sep=' ')\n",
    "\n",
    "# Calculate descriptive stats\n",
    "robyn_twitter_desc = descriptive_stats(robyn_twitter_str.split())\n",
    "robyn_twitter_desc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46294409",
   "metadata": {},
   "source": [
    "Q: How do you think the \"top 5 words\" would be different if we left stopwords in the data? \n",
    "\n",
    "A: If we left stopwords in the data, the top 5 words would have been more general, undescriptive words like I, the, and, so, etc. They would not have been able to identify the more meaningful words like love, music, and life as the top words if stopwords were present.\n",
    "\n",
    "---\n",
    "\n",
    "Q: What were your prior beliefs about the lexical diversity between the artists? Does the difference (or lack thereof) in lexical diversity between the artists conform to your prior beliefs? \n",
    "\n",
    "A: Prior to this assignment, I was not familar with Robyn as an artist, but I knew of Cher's songs. Based on the fact that the artists make music in a similar genre, I would assume that their lexical diversities would be similar. This assignment confirmed that their lexical diversities are pretty similar at around 0.1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4e1ac1",
   "metadata": {},
   "source": [
    "\n",
    "## Specialty Statistics\n",
    "\n",
    "The descriptive statistics we have calculated are quite generic. You will now calculate a handful of statistics tailored to these data.\n",
    "\n",
    "1. Ten most common emojis by artist in the twitter descriptions.\n",
    "1. Ten most common hashtags by artist in the twitter descriptions.\n",
    "1. Five most common words in song titles by artist. \n",
    "1. For each artist, a histogram of song lengths (in terms of number of tokens) \n",
    "\n",
    "We can use the `emoji` library to help us identify emojis and you have been given a function to help you.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "753a5a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(emoji.is_emoji(\"❤️\"))\n",
    "assert(not emoji.is_emoji(\":-)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986fc4c0",
   "metadata": {},
   "source": [
    "### Emojis 😁\n",
    "\n",
    "What are the ten most common emojis by artist in the twitter descriptions? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "269cd433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('❤', 79373), ('🌈', 47795), ('♥', 34239), ('🏳', 33591), ('✨', 29715), ('💙', 21498), ('🏻', 20970), ('🌊', 20319), ('✌', 16876), ('💜', 16607)]\n"
     ]
    }
   ],
   "source": [
    "# Cher Twitter Emojis\n",
    "\n",
    "# Find emojis in string\n",
    "emojis = [char for char in cher_twitter_str if emoji.is_emoji(char)]\n",
    "\n",
    "# Get counts for each emoji\n",
    "emoji_counts = Counter(emojis).most_common(10)\n",
    "\n",
    "# 10 most common\n",
    "print(emoji_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "27e7cdc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('❤', 4808), ('🌈', 4702), ('🏳', 3539), ('♥', 3117), ('✨', 2240), ('🏻', 1498), ('✌', 1194), ('🏼', 1145), ('♀', 848), ('💙', 818)]\n"
     ]
    }
   ],
   "source": [
    "# Robyn Twitter Emojis\n",
    "\n",
    "# Find emojis in string\n",
    "emojis = [char for char in robyn_twitter_str if emoji.is_emoji(char)]\n",
    "\n",
    "# Get counts for each emoji\n",
    "emoji_counts = Counter(emojis).most_common(10)\n",
    "\n",
    "# 10 most common\n",
    "print(emoji_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab9b770",
   "metadata": {},
   "source": [
    "### Hashtags\n",
    "\n",
    "What are the ten most common hashtags by artist in the twitter descriptions? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "b8e5cde9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'[', '/', '&', '\\\\', ':', '+', '=', '{', '-', '*', '~', '!', ']', '%', \"'\", '|', '\"', '$', '@', '(', '>', '.', '}', '<', '?', '`', '_', '^', ';', ')', ','}\n"
     ]
    }
   ],
   "source": [
    "# Remove hashtags from punctuation\n",
    "punctuation.discard('#')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "76b39768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning Function w/ keeping hashtags\n",
    "def data_cleaning_hashtag(text):\n",
    "    text = str(text)\n",
    "    \n",
    "    # Fold to lowercase\n",
    "    fold_lower = text.lower()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    remove_punctuation = ''.join(char for char in fold_lower if char not in punctuation)\n",
    "    \n",
    "    # Split on whitespace\n",
    "    split = remove_punctuation.split()\n",
    "    \n",
    "    \n",
    "    # Remove stopwords\n",
    "    clean_text = [word for word in split if word not in sw]\n",
    "    \n",
    "    return ' '.join(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "03163991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean twitter data with new cleaning function\n",
    "twitter_df['cleaned_description_#'] = twitter_df['description'].apply(data_cleaning_hashtag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "07c396f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('#resist', 10530), ('#blm', 9579), ('#blacklivesmatter', 7768), ('#theresistance', 3209), ('#fbr', 3129), ('#resistance', 2784), ('#1', 2424), ('#', 2139), ('#voteblue', 2060), ('#lgbtq', 1789)]\n"
     ]
    }
   ],
   "source": [
    "# Cher Twitter Hashtags\n",
    "\n",
    "# Subset cher twitter data\n",
    "cher_twitter = twitter_df[twitter_df['artist'] == 'cher']\n",
    "# Concat descriptions into one string\n",
    "cher_twitter_str = cher_twitter['cleaned_description_#'].str.cat(sep=' ')\n",
    "\n",
    "# Find hashtags in string\n",
    "hashtags = [word for word in cher_twitter_str.split() if word.startswith('#')]\n",
    "\n",
    "# Get counts for each hashtag\n",
    "hashtag_counts = Counter(hashtags).most_common(10)\n",
    "\n",
    "# 10 most common\n",
    "print(hashtag_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "f65c8ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('#blacklivesmatter', 584), ('#blm', 341), ('#music', 289), ('#1', 194), ('#', 169), ('#teamfollowback', 127), ('#edm', 108), ('#lgbtq', 81), ('#resist', 80), ('#art', 69)]\n"
     ]
    }
   ],
   "source": [
    "# Robyn Twitter Hashtags\n",
    "\n",
    "# Subset robyn twitter data\n",
    "robyn_twitter = twitter_df[twitter_df['artist'] == 'robyn']\n",
    "# Concat descriptions into one string\n",
    "robyn_twitter_str = robyn_twitter['cleaned_description_#'].str.cat(sep=' ')\n",
    "\n",
    "# Find hashtags in string\n",
    "hashtags = [word for word in robyn_twitter_str.split() if word.startswith('#')]\n",
    "\n",
    "# Get counts for each hashtag\n",
    "hashtag_counts = Counter(hashtags).most_common(10)\n",
    "\n",
    "# 10 most common\n",
    "print(hashtag_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10f21d5",
   "metadata": {},
   "source": [
    "### Song Titles\n",
    "\n",
    "What are the five most common words in song titles by artist? The song titles should be on the first line of the lyrics pages, so if you have kept the raw file contents around, you will not need to re-read the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb69b36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd4fd71",
   "metadata": {},
   "source": [
    "### Song Lengths\n",
    "\n",
    "For each artist, a histogram of song lengths (in terms of number of tokens). If you put the song lengths in a data frame with an artist column, matplotlib will make the plotting quite easy. An example is given to help you out. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805a1e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_replicates = 1000\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"artist\" : ['Artist 1'] * num_replicates + ['Artist 2']*num_replicates,\n",
    "    \"length\" : np.concatenate((np.random.poisson(125,num_replicates),np.random.poisson(150,num_replicates)))\n",
    "})\n",
    "\n",
    "df.groupby('artist')['length'].plot(kind=\"hist\",density=True,alpha=0.5,legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fde9ebb",
   "metadata": {},
   "source": [
    "Since the lyrics may be stored with carriage returns or tabs, it may be useful to have a function that can collapse whitespace, using regular expressions, and be used for splitting. \n",
    "\n",
    "Q: What does the regular expression `'\\s+'` match on? \n",
    "\n",
    "A: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e34516",
   "metadata": {},
   "outputs": [],
   "source": [
    "collapse_whitespace = re.compile(r'\\s+')\n",
    "\n",
    "def tokenize_lyrics(lyric) : \n",
    "    \"\"\"strip and split on whitespace\"\"\"\n",
    "    return([item.lower() for item in collapse_whitespace.split(lyric)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2294c440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your lyric length comparison chart here. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
